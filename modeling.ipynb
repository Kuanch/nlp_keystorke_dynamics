{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('DSL-StrongPasswordData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_channels, hidden_dim, num_layers=5, target_size=512):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(num_channels, hidden_dim, num_layers=num_layers, batch_first=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2target = nn.Linear(hidden_dim, target_size)\n",
    "\n",
    "    def forward(self, time_stamp):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            time_stamp: (num_feature, batch_size, num_channels)\n",
    "        Output:\n",
    "            metric: (target_size, batch_size)\n",
    "        \"\"\"\n",
    "        # h.shape = c.shape = (num_layers, num_feature, hidden_dim)\n",
    "        #                                     num_feature, batch_size, dim_vector\n",
    "        _, (h, c) = self.lstm(time_stamp.view(-1, len(time_stamp), self.num_channels).double())\n",
    "\n",
    "        # input the last h and c, whose shape = (num_feature, hidden_dim)\n",
    "        # shape = (2 * num_feature, hidden_dim) after concatenation\n",
    "        state_vector = torch.cat((h[-1, :, :], c[-1, :, :]), 0)\n",
    "        state_metric = self.hidden2target(state_vector)\n",
    "        metric = torch.sum(state_metric, dim=0)\n",
    "        return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize with num_channels per t and model depth\n",
    "lstm = LSTM(1, 128).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, repeat\n",
    "label = pd.Series(list(chain.from_iterable(zip(*repeat(list(range(51)), 400)))), dtype=np.int8, name='typist')\n",
    "training = data.iloc[:, 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke1 = torch.tensor(training.iloc[0].values).view(-1, 1, 1)\n",
    "stroke2 = torch.tensor(training.iloc[1].values).view(-1, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape should be (num_feature, batch_size, num_channels)\n",
    "embeddings = lstm(stroke1)\n",
    "embeddings2 = lstm(stroke2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512]),\n",
       " torch.Size([512]),\n",
       " tensor([-3.6684,  3.5893,  1.7412,  4.2320, -6.1728, -3.9881,  3.4944,  5.7145,\n",
       "          4.1667, -1.9314], dtype=torch.float64, grad_fn=<SliceBackward>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape, embeddings2.shape, embeddings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 distance\n",
    "sum(embeddings - embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 loss\n",
    "l1_loss = torch.nn.L1Loss(reduction='sum')\n",
    "loss = l1_loss(embeddings, embeddings2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sixigma/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "num_classes=51\n",
    "embeddings = lstm(stroke1)\n",
    "gt_label = torch.tensor(0)\n",
    "one_hot_label = torch.nn.functional.one_hot(gt_label, num_classes=num_classes)\n",
    "\n",
    "cls_head = torch.nn.Linear(len(embeddings), num_classes).double()\n",
    "softmax = torch.nn.Softmax()\n",
    "pred_cls = softmax(cls_head(embeddings).double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([51]),\n",
       " tensor([2.8560e-03, 2.7840e-03, 9.9352e-03, 3.6519e-03, 4.9495e-04, 8.2038e-04,\n",
       "         6.5720e-04, 7.3496e-02, 2.6416e-02, 4.5150e-03, 2.1842e-03, 1.6426e-03,\n",
       "         2.9907e-04, 7.4200e-04, 3.2409e-02, 6.0014e-04, 5.2635e-02, 6.0338e-02,\n",
       "         3.7767e-04, 5.9697e-04, 1.3123e-03, 6.8210e-02, 1.7910e-02, 3.5233e-03,\n",
       "         4.8046e-03, 2.9458e-03, 9.1984e-03, 1.7743e-02, 7.8691e-04, 8.6096e-04,\n",
       "         7.2602e-05, 6.3483e-03, 1.4384e-02, 6.0182e-02, 3.1188e-01, 7.4939e-04,\n",
       "         6.4176e-03, 1.1698e-02, 9.8340e-04, 9.0341e-02, 3.6891e-02, 6.8418e-04,\n",
       "         6.5804e-03, 1.7511e-02, 1.8775e-03, 4.3068e-03, 4.4076e-03, 1.8286e-02,\n",
       "         1.0695e-03, 2.4924e-04, 3.3405e-04], dtype=torch.float64,\n",
       "        grad_fn=<SoftmaxBackward>),\n",
       " torch.Size([51]),\n",
       " tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cls.shape, pred_cls, one_hot_label.shape, one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy loss\n",
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "loss = cross_entropy_loss(pred_cls.view(-1, num_classes), gt_label.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9522, dtype=torch.float64, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
