{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sixigma/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sixigma/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sixigma/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sixigma/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sixigma/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sixigma/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('DSL-StrongPasswordData.csv')\n",
    "\n",
    "time_stamp = data.iloc[:, 3:]\n",
    "# https://github.com/pytorch/pytorch/issues/2267#issuecomment-447923931\n",
    "np_time_stamp = np.array(time_stamp, dtype=np.float32)\n",
    "\n",
    "typist = np.zeros(data.shape[0])\n",
    "typist_list = list(set(data.subject))\n",
    "for i, s in data.subject.iteritems():\n",
    "    typist[i] = typist_list.index(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyStrokeDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A Map-style datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_stamp, typist, transform=None):\n",
    "        self.x = time_stamp\n",
    "        self.label = typist\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # for sampler\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        _x = self.x[idx, :]\n",
    "        _label = self.label[idx]\n",
    "        sample = {'x': _x, 'label': _label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        For a Iterable-style datasets.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = KeyStrokeDataset(np_time_stamp, typist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = torch.utils.data.RandomSampler(training)\n",
    "batch_sampler = torch.utils.data.BatchSampler(sampler, batch_size=192, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "        training, batch_sampler=batch_sampler, num_workers=4,\n",
    "        collate_fn=None, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[0.0747, 0.1746, 0.0999,  ..., 0.2746, 0.2147, 0.0631],\n",
      "        [0.0881, 0.1468, 0.0587,  ..., 0.2273, 0.1149, 0.0873],\n",
      "        [0.0937, 0.2509, 0.1572,  ..., 0.2723, 0.1923, 0.0803],\n",
      "        ...,\n",
      "        [0.0831, 0.1081, 0.0250,  ..., 0.1624, 0.0703, 0.1346],\n",
      "        [0.0964, 0.1230, 0.0266,  ..., 0.2198, 0.1461, 0.0576],\n",
      "        [0.0757, 0.3681, 0.2924,  ..., 0.8356, 0.7683, 0.0718]]), 'label': tensor([26., 27., 29., 33., 14., 39., 29.,  1.,  0., 41., 20., 45., 25.,  9.,\n",
      "        19., 11., 35., 12., 25., 50.,  7., 50., 41., 28., 28., 10., 46., 29.,\n",
      "        47., 28., 13., 12.,  0., 49., 40.,  7., 49., 10., 27.,  3., 19.,  6.,\n",
      "        37., 31., 27., 44.,  4., 42., 41., 40., 28., 31., 33.,  1., 39., 26.,\n",
      "        35., 32., 16.,  3.,  1., 22., 13.,  6., 48., 47., 50., 39., 34., 23.,\n",
      "        40.,  7., 10., 16.,  8., 32., 34., 15., 43.,  2.,  8., 22.,  9., 33.,\n",
      "        32., 27., 37., 21., 49., 29., 30., 29.,  2., 30., 25., 45., 49., 27.,\n",
      "        36., 44., 16.,  7., 42., 32.,  3.,  5., 34., 16.,  9., 31., 28., 10.,\n",
      "        40., 47., 35., 12., 13., 36., 39., 14., 13., 39.,  9.,  9.,  6.,  3.,\n",
      "        13.,  5.,  0., 23., 27., 34., 26., 20., 17.,  4., 46., 47.,  8., 34.,\n",
      "        40., 40.,  5., 21., 10.,  9., 40., 20., 45., 25., 18., 42., 13., 19.,\n",
      "        42., 38.,  1.,  5., 46., 41., 27., 36., 37.,  6., 34., 40., 33., 17.,\n",
      "        43., 34.,  2.,  5., 31., 17., 19., 29., 17., 10., 46., 39., 47., 29.,\n",
      "        10.,  4., 48., 10., 16., 30., 41., 35., 34., 38.], dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "for d in data_loader:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, num_feature, hidden_dim, num_classes):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(num_feature, hidden_dim)\n",
    "\n",
    "        self.hidden2class = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, time_stamp):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            time_stamp: (num_feature, batch_size, num_channels)\n",
    "        Output:\n",
    "            pred_class: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        h = self.input_layer(time_stamp.view(len(time_stamp), -1))\n",
    "        metric = self.hidden2class(h)\n",
    "        \n",
    "        return self.softmax(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(31, 50, num_layers=3, num_classes=51).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3,  3, 10, 44,  2, 10, 44, 44, 10,  9, 10, 44,  2, 44, 44, 10,  3, 44,\n",
      "        44, 44, 44, 44, 44, 36, 44, 10, 44, 44, 10,  3, 44, 44, 10, 10,  2, 44,\n",
      "         1, 10, 44, 10, 35,  2, 44, 10, 10, 44,  3, 10, 44, 44, 10,  3, 10, 44,\n",
      "        10,  3, 10, 44,  3, 10, 10, 44, 44, 44, 10,  3, 10, 10,  3, 10, 10, 35,\n",
      "        44, 35, 46,  3, 10, 10, 44, 10, 10,  3,  3, 10,  3, 44, 10,  3, 10, 10,\n",
      "        10, 44, 44, 35,  3, 44, 44, 10, 44, 10, 10, 44, 44, 44, 44, 10, 10,  2,\n",
      "        44, 10, 10, 10, 44, 10, 10, 44, 10, 44, 44, 44, 10, 10, 10, 10,  2,  2,\n",
      "        44, 10,  3,  3, 10, 36,  3, 35, 10, 44, 44, 10, 10, 44, 44, 44, 44, 35,\n",
      "        35, 10, 10, 10, 10, 10, 44, 10, 10, 10,  3, 44, 44,  2, 44, 44,  2, 10,\n",
      "         2, 10, 10, 44, 44, 10, 44,  3, 10, 10, 44, 10, 44, 30, 44, 44, 44, 10,\n",
      "        10, 44, 10, 10, 44, 44, 44, 10, 44, 44, 44,  6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/2267#issuecomment-447923931\n",
    "# RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM\n",
    "# data should be float32\n",
    "with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "        x = d['x'].to(device)\n",
    "        pred = model(x)\n",
    "        print(torch.argmax(pred, 1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    for d in data_loader:\n",
    "        x = d['x'].to(device)\n",
    "        labels = d['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        \n",
    "        loss = loss_fn(pred, labels.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([34, 42, 34, 19, 29, 34, 34,  6, 29,  0, 29,  6, 19, 42,  4,  6, 29, 29,\n",
      "        34,  4,  4, 29, 29, 19,  6, 34, 34, 29,  0,  6, 29, 34,  6, 29, 34, 34,\n",
      "        19,  6, 29, 29, 19,  4,  0,  4, 29, 42, 34, 34,  4, 34,  6,  4,  6,  0,\n",
      "         6, 19, 29, 29, 29, 34, 34, 29, 34,  0, 34,  4, 34,  4,  4, 34, 19, 29,\n",
      "        29, 29,  4,  6, 29, 29, 34,  6, 29,  6,  4,  0, 29, 29,  6, 34, 34,  0,\n",
      "        19, 29, 29, 34,  4,  4,  6, 34, 29, 29, 29, 34, 34, 19, 34,  6, 19,  0,\n",
      "        29,  6, 34, 34, 29, 29, 34,  4, 34, 29, 34, 29,  0, 19,  4, 29, 34, 34,\n",
      "        29,  4, 34,  0, 34, 34, 34, 29, 34, 29, 34, 34, 19, 29,  4, 19, 34, 34,\n",
      "        34,  6, 29,  6,  4, 29, 29, 29, 19, 19, 29,  4, 19, 34, 29, 19, 29, 29,\n",
      "        34, 29,  4,  6, 19, 34,  0,  4, 29, 19, 29,  4, 34, 19, 34, 19, 29,  6,\n",
      "        29, 34, 29,  6, 34, 29, 29, 34, 29, 19, 29,  0], device='cuda:0') tensor([15., 46., 31., 50., 31.,  5., 34., 37.,  2.,  0., 49.,  6., 19., 42.,\n",
      "        40.,  6., 22., 28., 25., 11., 11., 40.,  1., 15., 47., 24., 24., 40.,\n",
      "        33.,  6.,  5., 48., 37., 28.,  2.,  7., 21., 46., 23., 49., 19.,  8.,\n",
      "         0., 20., 39., 49., 16., 27., 49., 19.,  6., 50., 37.,  0., 33., 21.,\n",
      "        45.,  9., 29., 24., 34., 29., 28., 18., 49.,  4.,  3., 12., 47., 15.,\n",
      "        12., 29., 40., 23., 26.,  3.,  2., 44., 50., 49., 13.,  6.,  8., 37.,\n",
      "        43., 26., 39., 11., 49., 10., 39., 26., 22., 49.,  4.,  8.,  6., 30.,\n",
      "         2., 28., 43., 33., 43.,  7., 50., 49., 19., 20., 22., 21., 20., 21.,\n",
      "        31., 23., 46., 42., 42., 38., 43., 28.,  0., 35., 12., 36., 34., 18.,\n",
      "        31.,  4., 24., 42., 15.,  9., 21., 26.,  1., 44., 27., 47., 47., 33.,\n",
      "         5., 14.,  7.,  9., 16., 45., 28.,  6., 20., 11., 40., 31., 31., 47.,\n",
      "        31.,  8., 14., 24., 25., 18., 27., 26., 31., 13.,  4., 37., 14., 30.,\n",
      "         0.,  8., 39., 41.,  1., 42., 34., 26., 26., 19., 49.,  6.,  2., 30.,\n",
      "        29., 47., 18., 32., 32., 24., 36., 20.,  7.,  0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "        x = d['x'].to(device)\n",
    "        pred = model(x)\n",
    "        print(torch.argmax(pred, 1), d['label'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/')\n",
    "_step = 0\n",
    "epoch = 5\n",
    "data_loader_len = len(data_loader)\n",
    "for e in range(epoch):\n",
    "    for d in data_loader:\n",
    "        x = d['x'].to(device)\n",
    "        labels = d['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        \n",
    "        loss = loss_fn(pred, labels.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _step += 1\n",
    "        if _step % 100 == 0:\n",
    "            writer.add_scalar(\"Loss/train\", loss, e * data_loader_len + _step)\n",
    "\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        writer.add_histogram(\"parameter_{}\".format(i), p.data, e * data_loader_len + _step)\n",
    "        \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100,\n",
    "                                                       eta_min=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/')\n",
    "_step = 0\n",
    "epoch = 5\n",
    "data_loader_len = len(data_loader)\n",
    "for e in range(epoch):\n",
    "    for d in data_loader:\n",
    "        x = d['x'].to(device)\n",
    "        labels = d['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        \n",
    "        loss = loss_fn(pred, labels.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _step += 1\n",
    "        if _step % 100 == 0:\n",
    "            writer.add_scalar(\"Loss/train\", loss, e * data_loader_len + _step)\n",
    "            writer.add_scalar(\"Loss/lr\", scheduler.get_last_lr()[0], e * data_loader_len + _step)\n",
    "\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        writer.add_histogram(\"parameter_{}\".format(i), p.data, e * data_loader_len + _step)\n",
    "            \n",
    "    scheduler.step()\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/')\n",
    "_step = 0\n",
    "epoch = 5\n",
    "data_loader_len = len(data_loader)\n",
    "for e in range(epoch):\n",
    "    for d in data_loader:\n",
    "        x = d['x'].to(device)\n",
    "        labels = d['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        \n",
    "        loss = loss_fn(pred, labels.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _step += 1\n",
    "        if _step % 100 == 0:\n",
    "            writer.add_scalar(\"Loss/train\", loss, e * data_loader_len + _step)\n",
    "            writer.add_scalar(\"Loss/lr\", scheduler.get_last_lr()[0], e * data_loader_len + _step)\n",
    "\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        writer.add_histogram(\"parameter_{}\".format(i), p.data, e * data_loader_len + _step)\n",
    "            \n",
    "    scheduler.step()\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model/linear-{}.pt'.format(e))\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
